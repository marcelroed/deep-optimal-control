# <center>Meeting notes - 2019.08.30</center>

## Forberedning av spørsmål
### Faglig
* Tolkning av artikkelen
    * Analogi mellom nevralnett og løsning av en differensialligning
        * Stegene mellom lag i nettverket tolkes som tidssteg?
        * Antar dette teoretisk kan gjøres kontinuerlig
        * $f(u, t) = g_t(W_ty_{t-1} + b) \implies y_t = y_t + \Delta t g_t(W_ty_{t-1} + b)$, hvor $f$ er uttrykket for den deriverte i en ODE.
        * Vil lage en differensialligning som best mulig gir resultatene vi leter etter?
        * Steglengdene i integratoren virker som en hyperparameter
        * Er høyere ordens Runge Kutta metoder som å bruke større stride i et ResNet?
    * Invers lipschitz konstant, $\frac 1L$, som fast steglengde for gradient descent?
        * Dette er en ulik steglengde fra den som blir brukt i integratoren?
* Adjoint equation
    * Hva representerer $p$
    * Ressurser for å forstå adjoint equation
* Annen artikkel, Neural ODEs: https://arxiv.org/pdf/1806.07366.pdf

### Kode
* Hvordan fungerer den romlige transformasjonen på dataen som vises i de genererte plottene?
    * Er det anvendt nettverket helt til outputnoden, så brukt at red-blue bakgrunnen representerer hypotesefunksjonen?

#### Brynjulf
* HBVP i Brynjulf sin kode?

#### Matthias
* 

### Generalisering av trening av den variable skrittlengden
* Vil vi trene en funksjon av $t$?
* Skal jeg bruke backprop til å finne en derivert av loss mhp. $\overrightarrow{\Delta t}$?
* Skal hele nettverket trenes hver gang vi endrer $t$?

## Møte
